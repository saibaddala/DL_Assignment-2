{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BryBj72LUAy"
      },
      "outputs": [],
      "source": [
        "#  Imports & setup\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms, models\n",
        "import wandb\n",
        "\n",
        "# Fix seeds for deterministic behaviour (optional)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Quick GPU info for logs\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"[INFO] Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"[INFO] Training on CPU – may be slow.\")\n",
        "\n",
        "\n",
        "IMAGE_SIZE      = 224\n",
        "NUM_OF_CLASSES  = 10\n",
        "\n",
        "#  Helper functions\n",
        "\n",
        "def _get_tfms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
        "    \"\"\"Return deterministic (no augmentation) train/test transforms.\"\"\"\n",
        "    base = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
        "    to_t  = transforms.ToTensor()\n",
        "    return transforms.Compose([base, to_t]), transforms.Compose([base, to_t])\n",
        "\n",
        "\n",
        "def split_dataset_with_class_distribution(dataset, split_ratio):\n",
        "    \"\"\"\n",
        "    Split Nature‑12K into train/val by hard‑coded index ranges\n",
        "    so that class distribution mirrors original order (1 000 images/class).\n",
        "    \"\"\"\n",
        "    ranges = [(i * 1000, (i + 1) * 1000 - 1) for i in range(10)]\n",
        "    ranges[-1] = (9000, 9998)  # last class has 999 samples\n",
        "\n",
        "    train_ids, val_ids = [], []\n",
        "    for start, end in ranges:\n",
        "        idxs     = list(range(start, end + 1))\n",
        "        cut      = int(len(idxs) * split_ratio)\n",
        "        train_ids.extend(idxs[:cut])\n",
        "        val_ids.extend(idxs[cut:])\n",
        "\n",
        "    return Subset(dataset, train_ids), Subset(dataset, val_ids)\n",
        "\n",
        "\n",
        "def prepare_data(h_params: Dict) -> Dict:\n",
        "    \"\"\"Load datasets & build loaders. Prints split sizes for sanity.\"\"\"\n",
        "    train_tfms, test_tfms = _get_tfms()\n",
        "\n",
        "    base_dir = Path(\"/kaggle/input/nature1/inaturalist_12K\")\n",
        "    train_root, val_root = base_dir / \"train\", base_dir / \"val\"\n",
        "\n",
        "    full_train = ImageFolder(train_root, transform=train_tfms)\n",
        "    train_ds, val_ds = split_dataset_with_class_distribution(full_train, 0.8)\n",
        "    test_ds = ImageFolder(val_root, transform=test_tfms)  # Kaggle \"val\" as hold‑out test\n",
        "\n",
        "    bs = h_params[\"batch_size\"]\n",
        "    data = dict(\n",
        "        train_len=len(train_ds),\n",
        "        val_len=len(val_ds),\n",
        "        test_len=len(test_ds),\n",
        "        train_loader=DataLoader(train_ds, batch_size=bs, shuffle=True,  num_workers=2),\n",
        "        val_loader=  DataLoader(val_ds,   batch_size=bs, shuffle=False, num_workers=2),\n",
        "        test_loader= DataLoader(test_ds,  batch_size=bs, shuffle=False, num_workers=2),\n",
        "    )\n",
        "    print(f\"[INFO] train:{data['train_len']}  val:{data['val_len']}  test:{data['test_len']}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def _freeze_except_last_k(model: nn.Module, k: int) -> None:\n",
        "    \"\"\"Freeze all params then unfreeze last *k* parameters (flattened order).\"\"\"\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "    if k > 0:\n",
        "        for p in list(model.parameters())[-k:]:\n",
        "            p.requires_grad = True\n",
        "\n",
        "\n",
        "def resnet50Model(h_params: Dict) -> nn.Module:\n",
        "    \"\"\"Create a ResNet‑50 with custom head and selective unfreezing.\"\"\"\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, NUM_OF_CLASSES)\n",
        "    _freeze_except_last_k(model, h_params[\"last_unfreeze_layers\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Training & evaluation\n",
        "def train(h_params: Dict, data: Dict) -> None:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model  = resnet50Model(h_params).to(device)\n",
        "\n",
        "    # DataParallel allows multi‑GPU; safe on single GPU\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=h_params[\"learning_rate\"])\n",
        "\n",
        "    for ep in range(h_params[\"epochs\"]):\n",
        "        model.train()\n",
        "        ep_loss, ep_correct = 0.0, 0\n",
        "\n",
        "        for step, (x, y) in enumerate(data[\"train_loader\"]):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ep_loss    += loss.item()\n",
        "            ep_correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "            # verbose logging every 20 mini‑batches\n",
        "            if step % 20 == 0:\n",
        "                print(f\"Ep{ep:<2d} step{step:<4d} \"\n",
        "                      f\"batch_acc:{(out.argmax(1)==y).float().mean():.3f} \"\n",
        "                      f\"loss:{loss.item():.4f}\")\n",
        "\n",
        "        # ══ Validation ════════════════════════════════════════════════════\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in data[\"val_loader\"]:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out  = model(x)\n",
        "                val_loss    += criterion(out, y).item()\n",
        "                val_correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "        # Epoch‑level metrics\n",
        "        train_acc = ep_correct / data[\"train_len\"]\n",
        "        val_acc   = val_correct / data[\"val_len\"]\n",
        "        print(f\"[EP{ep}] train_acc:{train_acc:.4f} val_acc:{val_acc:.4f} \"\n",
        "              f\"train_loss:{ep_loss/len(data['train_loader']):.4f} \"\n",
        "              f\"val_loss:{val_loss/len(data['val_loader']):.4f}\")\n",
        "\n",
        "        # Send to W&B\n",
        "        wandb.log({\n",
        "            \"epoch\": ep,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_accuracy\":   val_acc,\n",
        "            \"train_loss\":     ep_loss / len(data[\"train_loader\"]),\n",
        "            \"val_loss\":       val_loss / len(data[\"val_loader\"]),\n",
        "        })\n",
        "\n",
        "        # free memory each epoch (mostly redundant)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ══ Save final weights with timestamp to avoid overwrite ═════════════\n",
        "    ts = int(time.time())\n",
        "    torch.save(model.state_dict(), f\"model_{ts}.pth\")\n",
        "    print(\"[DONE] Training complete – model saved.\")\n",
        "\n",
        "\n",
        "# Main execution\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    wandb.login()  # falls back to env var if key is set elsewhere\n",
        "    run_name = (f\"{h_params['model']}_ep_{h_params['epochs']}\"\n",
        "                f\"_bs_{h_params['batch_size']}\"\n",
        "                f\"_lr_{h_params['learning_rate']}\"\n",
        "                f\"_last_unfreeze_layers_{h_params['last_unfreeze_layers']}\")\n",
        "\n",
        "    run = wandb.init(project=\"DL Assignment 2B\", name=run_name, config=h_params)\n",
        "    data = prepare_data(h_params)\n",
        "    train(h_params, data)\n",
        "    run.finish()\n"
      ]
    }
  ]
}